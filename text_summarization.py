# -*- coding: utf-8 -*-
"""Text_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IUe1eoxwbjyjsJNqKGpLCr__snjyFjn5
"""

#####Text summerization########

#Text cleaning
#sentence Tokenization
#Word Tokenization
#Word-frequency table
#Summarization

Text= '''
Abstract—Deep learning-based models have been shown to
outperform human beings in many computer vision tasks with
massive available labeled training data in learning. However,
humans have an amazing ability to easily recognize images of
novel categories by browsing only a few examples of these categories.
In this case, few-shot learning comes into being to make
machines learn from extremely limited labeled examples. One
possible reason why human beings can well learn novel concepts
quickly and efficiently is that they have sufficient visual and
semantic prior knowledge. Toward this end, this work proposes
a novel knowledge-guided semantic transfer network (KSTNet)
for few-shot image recognition from a supplementary perspective
by introducing auxiliary prior knowledge. The proposed network
jointly incorporates vision inferring, knowledge transferring,
and classifier learning into one unified framework for optimal
compatibility. A category-guided visual learning module is
developed in which a visual classifier is learned based on the
feature extractor along with the cosine similarity and contrastive
loss optimization. To fully explore prior knowledge of category
correlations, a knowledge transfer network is then developed
to propagate knowledge information among all categories to
learn the semantic-visual mapping, thus inferring a knowledgebased
classifier for novel categories from base categories. Finally,
we design an adaptive fusion scheme to infer the desired
classifiers by effectively integrating the above knowledge and
visual information. Extensive experiments are conducted on two
widely used Mini-ImageNet and Tiered-ImageNet benchmarks to
validate the effectiveness of KSTNet. Compared with the state
of the art, the results show that the proposed method achieves
favorable performance with minimal bells and whistles, especially
in the case of one-shot learning.
Index Terms—Few-shot learning, image recognition, knowledge
graph, knowledge transfer.
'''

!pip install -U spacy
!python -m spacy download en_core_web_sm

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

stopwords = list(STOP_WORDS)

#creating a model
nlp = spacy.load('en_core_web_sm')

#tokenization
doc = nlp(Text)

tokens = [token.text for token in doc]
print(tokens)
#includes both stopwords and punctuation

punctuation = punctuation + '\n'
punctuation

word_freq = {}
 for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in word_freq.keys():
         word_freq[word.text] = 1
      else:
        word_freq[word.text] += 1

print(word_freq)

max_freq = max(word_freq.values())

max_freq

for word in word_freq.keys():
  word_freq[word] = word_freq[word]/max_freq

#divided each word by max freq to get normalised freq b/w 0 & 1

print(word_freq)

#Sentence Tokenization

sentence_tokens = [sent for sent in doc.sents]
print(sentence_tokens)

# calculate sentence score
sentence_scores = {}
for sent in sentence_tokens:
  for word in sent:
    if word.text.lower() in word_freq.keys():
      if sent not in sentence_scores.keys():
        sentence_scores[sent] = word_freq[word.text.lower()]
      else:
        sentence_scores[sent] += word_freq[word.text.lower()]

sentence_scores

from heapq import nlargest

select_len = int(len(sentence_tokens) * 0.3)
select_len

summary = nlargest(select_len, sentence_scores, key = sentence_scores.get)

summary

final_summary = [word.text for word in summary]

summary = ''.join(final_summary)

summary

len(Text)

len(summary)

#Extractive