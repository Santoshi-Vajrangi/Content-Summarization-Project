# -*- coding: utf-8 -*-
"""scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WttDYpwZxvulOaYjDhMyBvOydn_Nfkfi
"""

!pip install --upgrade google-api-python-client

!pip install rouge

import os, sys
import configparser
re="\033[1;31m"
gr="\033[1;32m"
cy="\033[1;36m"

print(gr+"[+] Installing requierments ...")
os.system('python -m pip install pandas')
os.system('pip install apiclient')
os.system('pip install --upgrade google-api-python-client')

os.system("touch config.data")
cpass = configparser.RawConfigParser()
cpass.add_section('cred')

xhash = input(gr+"[+] Enter Video ID : "+re)
cpass.set('cred', 'hash', xhash)
setup = open('config.data', 'w')
cpass.write(setup)
setup.close()
print(gr+"[+] Setup complete!")
print(gr+"[+] Now you can run YouTube Scraper!")

!pip install rouge_score



import os
import google.generativeai as palm


from apiclient.discovery import build
import pandas as pd
import random
from rouge_score import rouge_scorer

# Set your YouTube API key and Google API key
YOUTUBE_API_KEY = "AIzaSyDVKROa2PHT7JrXg_bMqZ1-7HNCnxpqsZ8"
GOOGLE_API_KEY = "AIzaSyDxp5B2tqKHGOWjI0rp8pL1eyzJMiIdVac"

# Initialize the YouTube API
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# Get video ID from the user
ID = input("Enter the YouTube video ID: ")

List = [['Name', 'Comment', 'Likes', 'Time', 'Reply Count', 'Video Description']]

class PalmAI(object):
    def __init__(self, text_content):
        self.text_content = text_content
        self.generated_summary = ""

    def GenerateSummary(self):
        palm.configure(api_key=GOOGLE_API_KEY)
        request = f"from the given comments summarize what the people are talking about.Also give the general tone of the comments.Give the output within 250 words : {self.text_content}"
        response = palm.generate_text(prompt=request)
        self.generated_summary = response.result

def get_video_description(video_id):
    video_data = youtube.videos().list(part='snippet', id=video_id).execute()
    if 'items' in video_data:
        return video_data['items'][0]['snippet']['description']
    return ''

def calculate_rouge(hypothesis, reference):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(hypothesis, reference)
    return scores

def scrape_all_with_replies():
    video_description = get_video_description(ID)  # Get video description

    data = youtube.commentThreads().list(part='snippet', videoId=ID, maxResults=100, textFormat="plainText").execute()

    comments_to_concatenate = []  # Initialize a list to store random comments

    for i in data["items"]:
        name = i["snippet"]['topLevelComment']["snippet"]["authorDisplayName"]
        comment = i["snippet"]['topLevelComment']["snippet"]["textDisplay"]
        likes = i["snippet"]['topLevelComment']["snippet"]['likeCount']
        published_at = i["snippet"]['topLevelComment']["snippet"]['publishedAt']
        replies = i["snippet"]['totalReplyCount']

        List.append([name, comment, likes, published_at, replies, video_description])

        # Add comments to the list for later concatenation
        comments_to_concatenate.append(comment)

        TotalReplyCount = i["snippet"]['totalReplyCount']

        if TotalReplyCount > 0:
            parent = i["snippet"]['topLevelComment']["id"]

            data2 = youtube.comments().list(part='snippet', maxResults=100, parentId=parent, textFormat="plainText").execute()

            for i in data2["items"]:
                name = i["snippet"]["authorDisplayName"]
                comment = i["snippet"]["textDisplay"]
                likes = i["snippet"]['likeCount']
                published_at = i["snippet"]['publishedAt']
                replies = ""

                List.append([name, comment, likes, published_at, replies, video_description])

                # Add comments to the list for later concatenation
                comments_to_concatenate.append(comment)

    # Randomly select 20 comments from the list and concatenate them into a single string
        selected_comments = random.sample(comments_to_concatenate, min(20, len(comments_to_concatenate)))
        concatenated_comments = ' '.join(selected_comments)
        summaryGenerator = PalmAI(concatenated_comments)
        summaryGenerator.GenerateSummary()

    df = pd.DataFrame({'Name': [i[0] for i in List], 'Comment': [i[1] for i in List], 'Likes': [i[2] for i in List],
                       'Time': [i[3] for i in List], 'Reply Count': [i[4] for i in List],
                       'Video Description': [i[5] for i in List], 'Summary': summaryGenerator.generated_summary})

    df.to_csv('YT-Scrape-Result14.csv', index=False, header=True)  # Set header=True to include headers

     # Calculate ROUGE score
    reference_summary = "The people are discussing about Tailwind CSS. Some of them think that it is not necessary to learn Tailwind CSS and people can directly use it by reading the documentation. Others disagree and think that it is better to learn Tailwind CSS first. The overall tone of the comments is positive and helpful."  # Provide a reference summary for comparison
    rouge_scores = calculate_rouge(summaryGenerator.generated_summary, reference_summary)
    print("ROUGE Scores:", rouge_scores)

    return "Successful! Check the CSV file that you have just created. Concatenated Comments: " + concatenated_comments

scrape_all_with_replies()

import pandas as pd
from palm_llm import PalmLLM

# Load the PalmLLM model
model = PalmLLM()

# Read the CSV file
df = pd.read_csv('YT-Scrape-Result8.csv')

# Summarize the data
summaries = []
for i in range(len(df)):
    summary = model.summarize(df.iloc[i]['Comment'])
    summaries.append(summary)

# Add the summaries to the DataFrame
df['Summary'] = summaries

# Save the updated DataFrame to a new CSV file
df.to_csv('YT-Scrape-Result8-Summarized.csv', index=False)

print('Successfully summarized the data and stored it in the CSV file YT-Scrape-Result8-Summarized.csv')

!pip install -q google-generativeai

import os

os.environ["GoogleApiKey"] = "AIzaSyDxp5B2tqKHGOWjI0rp8pL1eyzJMiIdVac"

import pprint
import google.generativeai as palm



import os
import google.generativeai as palm
from apiclient.discovery import build
import pandas as pd
import random
from rouge_score import rouge_scorer


# Set your YouTube API key and Google API key
YOUTUBE_API_KEY = "AIzaSyDVKROa2PHT7JrXg_bMqZ1-7HNCnxpqsZ8"
GOOGLE_API_KEY = "AIzaSyDxp5B2tqKHGOWjI0rp8pL1eyzJMiIdVac"

# Initialize the YouTube API
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# Get video ID from the user
ID = input("Enter the YouTube video ID: ")

List = [['Name', 'Comment', 'Likes', 'Time', 'Reply Count', 'Video Description', 'Summary']]

class PalmAI(object):
    def __init__(self, text_content):
        self.text_content = text_content
        self.generated_summary = ""

    def GenerateSummary(self):
        palm.configure(api_key=GOOGLE_API_KEY)
        request = f"from the given comments summarize what the people are talking about. Also, give the general tone of the comments. Give the output within 250 words: {self.text_content}"
        response = palm.generate_text(prompt=request)
        self.generated_summary = response.result

def get_video_description(video_id):
    video_data = youtube.videos().list(part='snippet', id=video_id).execute()
    if 'items' in video_data:
        return video_data['items'][0]['snippet']['description']
    return ''

def calculate_rouge(hypothesis, reference):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(hypothesis, reference)
    return scores

def scrape_all_with_replies():
    video_description = get_video_description(ID)  # Get video description

    data = youtube.commentThreads().list(part='snippet', videoId=ID, maxResults=100, textFormat="plainText").execute()

    comments_to_concatenate = []  # Initialize a list to store random comments

    for i in data["items"]:
        name = i["snippet"]['topLevelComment']["snippet"]["authorDisplayName"]
        comment = i["snippet"]['topLevelComment']["snippet"]["textDisplay"]
        likes = i["snippet"]['topLevelComment']["snippet"]['likeCount']
        published_at = i["snippet"]['topLevelComment']["snippet"]['publishedAt']
        replies = i["snippet"]['totalReplyCount']

        # Add comments to the list for later concatenation
        comments_to_concatenate.append(comment)

        TotalReplyCount = i["snippet"]['totalReplyCount']

        if TotalReplyCount > 0:
            parent = i["snippet"]['topLevelComment']["id"]

            data2 = youtube.comments().list(part='snippet', maxResults=100, parentId=parent, textFormat="plainText").execute()

            for i in data2["items"]:
                comment = i["snippet"]["textDisplay"]

                # Add comments to the list for later concatenation
                comments_to_concatenate.append(comment)

    # Randomly select 20 comments from the list and concatenate them into a single string
    selected_comments = random.sample(comments_to_concatenate, min(20, len(comments_to_concatenate)))
    concatenated_comments = ' '.join(selected_comments)

    summaryGenerator = PalmAI(concatenated_comments)
    summaryGenerator.GenerateSummary()

    # Append the summary to the list
    List.append([name, comment, likes, published_at, replies, video_description, summaryGenerator.generated_summary])

    # Print the summary
    print("Generated Summary:", summaryGenerator.generated_summary)

    # Calculate ROUGE score
    reference_summary = "The people are discussing about Tailwind CSS. Some of them think that it is not necessary to learn Tailwind CSS and people can directly use it by reading the documentation. Others disagree and think that it is better to learn Tailwind CSS first. The overall tone of the comments is positive and helpful."  # Provide a reference summary for comparison
    rouge_scores = calculate_rouge(summaryGenerator.generated_summary, reference_summary)
    print("ROUGE Scores:", rouge_scores)

    return "Successful! Summary: " + summaryGenerator.generated_summary

scrape_all_with_replies()





